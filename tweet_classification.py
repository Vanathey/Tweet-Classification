# -*- coding: utf-8 -*-
"""s19813_prog_assignment02.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Tvtrgpu-yto0DTi4raiskQOUoZKfjy14
"""

# LSTM and Bi-LSTM tweet classification without pre-trained embeddings

import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
# Change imports for Tokenizer and pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout

# Load data
train_df = pd.read_csv("phm_train.csv")
test_df = pd.read_csv("phm_test.csv")

# Clean text function
def clean_text(text):
    text = re.sub(r'http\S+', '', text)  # Remove URLs
    text = re.sub(r'@\w+', '', text)     # Remove mentions
    text = re.sub(r'#\w+', '', text)     # Remove hashtags
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove special chars
    text = text.lower().strip()
    return text

train_df['tweet'] = train_df['tweet'].astype(str).apply(clean_text)
test_df['tweet'] = test_df['tweet'].astype(str).apply(clean_text)

# Tokenization
tokenizer = Tokenizer(num_words=10000, oov_token="<OOV>")
tokenizer.fit_on_texts(train_df['tweet'])

X_train_seq = tokenizer.texts_to_sequences(train_df['tweet'])
X_test_seq = tokenizer.texts_to_sequences(test_df['tweet'])

# Padding
maxlen = 100
X_train_pad = pad_sequences(X_train_seq, maxlen=maxlen, padding='post')
X_test_pad = pad_sequences(X_test_seq, maxlen=maxlen, padding='post')

y_train = train_df['label'].values
y_test = test_df['label'].values

# Function to build LSTM model
def build_lstm():
    model = Sequential()
    model.add(Embedding(input_dim=10000, output_dim=64, input_length=maxlen))
    model.add(LSTM(64))
    model.add(Dropout(0.5))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

# Function to build Bi-LSTM model
def build_bilstm():
    model = Sequential()
    model.add(Embedding(input_dim=10000, output_dim=64, input_length=maxlen))
    model.add(Bidirectional(LSTM(64)))
    model.add(Dropout(0.5))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

# Train and evaluate function
def train_and_evaluate(model, name):
    print(f"\nTraining {name} model...")
    history = model.fit(X_train_pad, y_train, validation_split=0.2, epochs=5, batch_size=64)
    y_pred = (model.predict(X_test_pad) > 0.5).astype(int)
    acc = accuracy_score(y_test, y_pred)
    print(f"\n{name} Test Accuracy: {acc:.4f}")
    print(classification_report(y_test, y_pred))


    return history, acc

# Train LSTM
lstm_model = build_lstm()
lstm_history, lstm_acc = train_and_evaluate(lstm_model, "LSTM")

# Train Bi-LSTM
bilstm_model = build_bilstm()
bilstm_history, bilstm_acc = train_and_evaluate(bilstm_model, "Bi-LSTM")

# Plot comparison
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(lstm_history.history['accuracy'], label='LSTM Train')
plt.plot(lstm_history.history['val_accuracy'], label='LSTM Val')
plt.plot(bilstm_history.history['accuracy'], label='Bi-LSTM Train')
plt.plot(bilstm_history.history['val_accuracy'], label='Bi-LSTM Val')
plt.title('Accuracy Comparison')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(lstm_history.history['loss'], label='LSTM Train')
plt.plot(lstm_history.history['val_loss'], label='LSTM Val')
plt.plot(bilstm_history.history['loss'], label='Bi-LSTM Train')
plt.plot(bilstm_history.history['val_loss'], label='Bi-LSTM Val')
plt.title('Loss Comparison')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.savefig("model_comparison.png")
plt.show()